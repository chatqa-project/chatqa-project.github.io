<html>
  <head>
    <meta charvctk_set="UTF-8">
    <title>Introducing ChatQA-1.5</title>
    <style type="text/css">

      /* css for table */
      .tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;
        font-family:Arial, sans-serif;font-size:13px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;
        font-family:Arial, sans-serif;font-size:13px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg .tg-baqh{text-align:center;vertical-align:top}

      /* css for text container */
      .container {
        margin: 0 auto; /* Center the container horizontally */
        max-width: 1150px; /* Set the maximum width of the container */
        text-align: left; /* Left-align the text within the container */
        padding: 0 20px; /* Optional: Add padding to the container */
        line-height: 1.5; /* Adjust line height for spacing between lines */
        font-size: 22px; /* Set the font size for the container */
      }
      
      /* color for subtitle */
      .colored_subtitle {
        color: rgb(91, 91, 91); /* Set the color of the text */
      }

      figure img {
        max-width: 90%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }
      
    </style>
  </head>
  <div class="container">

    <h1>Introducing ChatQA-1.5: Outperforming GPT-4 at RAG-based Conversational Question Answering</h1>
    <h3><a href="https://arxiv.org/abs/2401.10225">Link to Paper (arXiv)</a> &ensp; <a href="">Link to Models</a> &ensp; <a href="">Link to Data</a> </h3>
    <p>
    Today, we release ChatQA-1.5-8b and ChatQA-1.5-70b models, which excels at RAG-based conversational question answering (QA). ChatQA-1.5 is built using the recipe from <a href="https://arxiv.org/abs/2401.10225">ChatQA (1.0)</a> on top of Llama-3 foundation model. Additionally, we incorporate more conversational QA data to enhance its tabular and arithmatic calculation capability.
    </p>

    <h2>ConvRAG Bench</h2>
    We introduce ConvRAG Bench: a benchmark for retrieval/context-augmented conversational QA evaluation. ConvRAG Bench consisting of 10 datasets: <a href="https://arxiv.org/abs/2011.06623">Doc2Dial</a>, QuAC, QReCC, TopioCQA, INSCIT, CoQA, HybriDial, DoQA, SQA, ConvFinQA, which covers both long and short text-based documents, tabular reasoning and arithmatic calculations. 
    <br>
    ConvRAG Bench also includes evaluations for the unanswerable scenario, where we evaluate models' capability to tell whether questions are answerable based on the given context.

    <!-- tables for the ConvRAG Bench -->
    <h3 class="colored_subtitle">Main Results</h3>
    <figure>
      <img src="figs/Average_Scores_ConvRAG_Bench.png">
    </figure>

    <h3 class="colored_subtitle">Unanswerable Scenario Evaluation</h3>
    
    <table class="tg">
      <thead>
        <tr>
          <th class="tg-baqh"></th>
          <th class="tg-baqh">&nbsp;&nbsp;GPT-3.5-turbo-0613&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;Command-R-Plus&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;Llama-3-instruct-70b&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;GPT-4-0613&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;ChatQA-1.0-70B&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;ChatQA-1.5-8B&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;ChatQA-1.5-70B&nbsp;&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="tg-baqh">&nbsp;&nbsp;Average&nbsp;&nbsp;</td>
          <td class="tg-baqh">73.27</td>
          <td class="tg-baqh">68.11</td>
          <td class="tg-baqh">76.42</td>
          <td class="tg-baqh">80.73</td>
          <td class="tg-baqh">77.25</td>
          <td class="tg-baqh">75.57</td>
          <td class="tg-baqh">71.86</td>
        </tr>
      </tbody>
    </table>

  <h2>Other Evaluations</h2>
    <h3 class="colored_subtitle">KILT</h3>
    <table class="tg">
      <thead>
        <tr>
          <th class="tg-baqh"></th>
          <th class="tg-baqh">&nbsp;&nbsp;&nbsp;Command-R-Plus&nbsp;&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;&nbsp;Llama3-instruct-70b&nbsp;&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;&nbsp;ChatQA-1.5-70B-v28-9&nbsp;&nbsp;&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="tg-baqh">&nbsp;&nbsp;Average&nbsp;&nbsp;</td>
          <td class="tg-baqh">71.5</td>
          <td class="tg-baqh">67.57</td>
          <td class="tg-baqh">69.75</td>
        </tr>
      </tbody>
    </table>

    <h3 class="colored_subtitle">Human Evaluations</h3>

    <h2>Training Datasets</h2>
    We release <a href="">training datasets</a> which include our conversational QA dataset and single-turn QA datasets.

    <h2>Conversational QA Retriever</h2>
    We release conversational QA retriever

    <h2>License</h2>

    <h2>Citation</h2>


  </div>
  

</html>


