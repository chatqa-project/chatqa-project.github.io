<html>
  <head>
    <meta charvctk_set="UTF-8">
    <title>Introducing ChatQA-1.5</title>
    <style>
      .centered-text {
        text-align: center; /* Horizontally center the text */
        margin-top: 50px; /* Adjust as needed for vertical centering */
      }
    </style>
  </head>
  <div class="centered-text">

    <h2>Introducing ChatQA-1.5: Outperforming GPT-4 at Conversational Question Answering</h2>
    <h3><a href="https://arxiv.org/abs/2401.10225">Link to Paper (arXiv)</a> &ensp; <a href="">Link to Models</a> &ensp; <a href="">Link to Data</a> </h3>
    <p>
    Today, we release ChatQA-1.5-8b and ChatQA-1.5-70b models, which excels at RAG-based conversational question answering (QA). ChatQA-1.5 is built using the recipe from <a href="https://arxiv.org/abs/2401.10225">ChatQA (1.0)</a> on top of Llama-3 foundation model. Additionally, we incorporate more conversational QA data to enhance its tabular and arithmatic calculation capability.
    </p>

    <h3>ConvRAG Bench</h3>
    We introduce ConvRAG Bench: a benchmark for retrieval/context-augmented conversational QA evaluation. ConvRAG Bench consisting of 10 datasets: <a href="https://arxiv.org/abs/2011.06623">Doc2Dial</a>, QuAC, QReCC, TopioCQA, INSCIT, CoQA, HybriDialogue, DoQA, SQA, ConvFinQA, which covers both long and short text-based documents, tabular reasoning and arithmatic calculations.

    <!-- tables for the ConvRAG Bench -->
    <table>
      <thead>
        <tr>
          <th></th>
          <th>ChatQA-1.0-7B</th>
          <th>Command-R-Plus</th>
          <th>Llama-3-instruct-70b</th>
          <th>GPT-4-0613</th>
          <th>ChatQA-1.0-70B</th>
          <th><span style="font-weight:bold">ChatQA-1.5-8B</span></th>
          <th><span style="font-weight:bold">ChatQA-1.5-70B</span></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Doc2Dial</td>
          <td>37.88</td>
          <td>33.51</td>
          <td>37.88</td>
          <td>34.16</td>
          <td>38.9</td>
          <td>39.33</td>
          <td>41.26</td>
        </tr>
        <tr>
          <td>QuAC</td>
          <td>29.69</td>
          <td>34.16</td>
          <td>36.96</td>
          <td>40.29</td>
          <td>41.82</td>
          <td>39.73</td>
          <td>38.82</td>
        </tr>
        <tr>
          <td>QReCC</td>
          <td>46.97</td>
          <td>49.77</td>
          <td>51.34</td>
          <td>52.01</td>
          <td>48.05</td>
          <td>49.03</td>
          <td>51.4</td>
        </tr>
        <tr>
          <td>CoQA</td>
          <td>76.61</td>
          <td>69.71</td>
          <td>76.98</td>
          <td>77.42</td>
          <td>78.57</td>
          <td>76.46</td>
          <td>78.44</td>
        </tr>
        <tr>
          <td>DoQA</td>
          <td>41.57</td>
          <td>40.67</td>
          <td>41.24</td>
          <td>43.39</td>
          <td>51.94</td>
          <td>49.6</td>
          <td>50.67</td>
        </tr>
        <tr>
          <td>ConvFinQA</td>
          <td>51.61</td>
          <td>71.21</td>
          <td>76.6</td>
          <td>81.28</td>
          <td>73.69</td>
          <td>78.46</td>
          <td>81.88</td>
        </tr>
        <tr>
          <td>SQA</td>
          <td>61.87</td>
          <td>74.07</td>
          <td>69.61</td>
          <td>79.21</td>
          <td>69.14</td>
          <td>73.28</td>
          <td>83.82</td>
        </tr>
        <tr>
          <td>TopioCQA</td>
          <td>45.45</td>
          <td>53.77</td>
          <td>49.72</td>
          <td>45.09</td>
          <td>50.98</td>
          <td>49.96</td>
          <td>55.63</td>
        </tr>
        <tr>
          <td><span style="font-style:italic">HybriDialogue*</span></td>
          <td>54.51</td>
          <td>46.7</td>
          <td>48.59</td>
          <td>49.81</td>
          <td>56.44</td>
          <td>65.76</td>
          <td>68.27</td>
        </tr>
        <tr>
          <td>INSCIT</td>
          <td>30.96</td>
          <td>35.76</td>
          <td>36.23</td>
          <td>36.34</td>
          <td>31.9</td>
          <td>30.1</td>
          <td>32.31</td>
        </tr>
        <tr>
          <td>average (all)</td>
          <td>47.71</td>
          <td>50.93</td>
          <td>52.52</td>
          <td>53.90</td>
          <td>54.14</td>
          <td>55.17</td>
          <td><span style="font-weight:bold">58.25</span></td>
        </tr>
        <tr>
          <td>average (except HybriDialogue)</td>
          <td>46.96</td>
          <td>51.40</td>
          <td>52.95</td>
          <td>54.35</td>
          <td>53.89</td>
          <td>53.99</td>
          <td><span style="font-weight:bold">57.14</span></td>
        </tr>
      </tbody>
    </table>

    ConvRAG Bench also includes evaluations for unanswerable scenarios
    
    <h3>Training Datasets</h3>
    We release <a href="">training datasets</a> which include our conversational QA dataset and single-turn QA datasets.

    <h3>Conversational QA Retriever</h3>
    We release conversational QA retriever

    <h3>License</h3>

    <h3>Citation</h3>


  </div>
  

</html>


