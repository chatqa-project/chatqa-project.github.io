<html>
  <head>
    <meta charvctk_set="UTF-8">
    <title>Introducing ChatQA-1.5</title>
    <style type="text/css">

      /* css for table */
      .tg  {border-collapse:collapse;border-color:#9ABAD9;border-spacing:0;}
      .tg td{background-color:#EBF5FF;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#444;
        font-family:Arial, sans-serif;font-size:12px;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg th{background-color:#409cff;border-color:#9ABAD9;border-style:solid;border-width:1px;color:#fff;
        font-family:Arial, sans-serif;font-size:12px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
      .tg .tg-baqh{text-align:center;vertical-align:top}

      /* css for text container */
      .container {
        margin: 0 auto; /* Center the container horizontally */
        max-width: 900px; /* Set the maximum width of the container */
        text-align: left; /* Left-align the text within the container */
        padding: 0 20px; /* Optional: Add padding to the container */
        line-height: 1.5; /* Adjust line height for spacing between lines */
        font-size: 18px; /* Set the font size for the container */
      }
      
      /* color for subtitle */
      .colored_subtitle {
        color: rgb(91, 91, 91); /* Set the color of the text */
      }

      .custom-link {
            color: #76b900; /* Sets the color to blue */
            text-decoration: none; /* Removes underline */
      }

      .img-chatrag {
        max-width: 90%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      .img-humaneval {
        max-width: 55%; /* Set the maximum width of the image to 100% */
        height: auto; /* Ensure the aspect ratio of the image is maintained */
      }

      pre {
        background-color: #f4f4f4;
        padding: 10px;
        border-radius: 5px;
        font-size: 15px;
        overflow-x: auto;
      }
      code {
        display: block;
      }
      
    </style>
  </head>

  <div class="container">
    <br>
    <h2>Introducing ChatQA-1.5: Surpassing GPT-4 on Conversational QA and RAG</h2>
    <h4> <a href="https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B" class="custom-link">Model WeightsðŸ¤—</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/ChatRAG-Bench" class="custom-link">Evaluation DataðŸ¤—</a> &ensp; <a href="https://huggingface.co/datasets/nvidia/ChatQA-Training-Data" class="custom-link">Training DataðŸ¤—</a> &ensp; <a href="https://huggingface.co/nvidia/dragon-multiturn-query-encoder" class="custom-link">RetrieverðŸ¤—</a> &ensp; <a href="https://arxiv.org/abs/2401.10225" class="custom-link">Paper</a>  </h4>
    <p>
    Today, we release ChatQA-1.5, which excels at conversational question answering (QA) and retrieval-augmented generation (RAG). ChatQA-1.5 is developed using an improved training recipe from <a href="https://arxiv.org/abs/2401.10225" class="custom-link">ChatQA-1.0</a>, and it is built on the top of the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" class="custom-link">Llama-3 base model</a>. Specifically, we incorporate more conversational QA data to enhance its tabular and arithmetic calculation capability. ChatQA-1.5 has two variants: Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B. We share the model weights, evaluation data, training data, and SFT recipe for future study.
    </p>

    <h3>ChatRAG Bench</h3>
    We release <a href="https://huggingface.co/datasets/nvidia/ChatRAG-Bench" class="custom-link">ChatRAG Bench</a>: a benchmark for evaluating a model's conversational QA capability over documents or retrieved context. 
    ChatRAG Bench consisting of 10 datasets: <a href="https://arxiv.org/abs/2011.06623" class="custom-link">Doc2Dial</a>, <a href="https://arxiv.org/abs/1808.07036" class="custom-link">QuAC</a>, <a href="https://arxiv.org/abs/2010.04898" class="custom-link">QReCC</a>, <a href="https://arxiv.org/abs/2110.00768" class="custom-link">TopioCQA</a>, <a href="https://arxiv.org/abs/2207.00746" class="custom-link">INSCIT</a>, <a href="https://arxiv.org/abs/1808.07042" class="custom-link">CoQA</a>, <a href="https://arxiv.org/abs/2204.13243" class="custom-link">HybriDialogue</a>, <a href="https://arxiv.org/abs/2005.01328" class="custom-link">DoQA</a>, <a href="https://aclanthology.org/P17-1167/" class="custom-link">SQA</a>, <a href="https://arxiv.org/abs/2210.03849" class="custom-link">ConvFinQA</a>. 
    ChatRAG Bench covers a wide range of documents and question types, which require models to generate responses from long context, comprehend and reason over tables, conduct arithmetic calculations, and indicate when questions cannot be found within the context.

    <!-- tables for the ChatRAG Bench -->
    <h4 class="colored_subtitle">Superior Accuracy on RAG and Conversational QA</h4>
    <figure>
      <img class="img-chatrag" src="figs/Average_Scores_ChatRAG_Bench.png">
    </figure>
    We use the same (retrieved) context as inputs for all the LLMs for a fair comparison. Both Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B achieve better average scores (e.g, unigram F1) than GPT-4-0613 and Command-R-Plus (100B).
    
    <h4 class="colored_subtitle">Evaluation of Unanswerable Scenario</h4>
    ChatRAG Bench also includes evaluations for the unanswerable scenario, where we evaluate models' capability to determine whether the answer to the question can be found within the given context. Equipping models with such capability can substantially decrease the likelihood of hallucination.

    <table class="tg">
      <thead>
        <tr>
          <th class="tg-baqh"></th>
          <th class="tg-baqh">&nbsp;&nbsp;GPT-3.5-turbo-0613&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;Command-R-Plus&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;Llama-3-instruct-70b&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;GPT-4-0613&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;ChatQA-1.0-70B&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;ChatQA-1.5-8B&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;ChatQA-1.5-70B&nbsp;&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="tg-baqh">&nbsp;&nbsp;Average&nbsp;&nbsp;</td>
          <td class="tg-baqh">73.27</td>
          <td class="tg-baqh">68.11</td>
          <td class="tg-baqh">76.42</td>
          <td class="tg-baqh">80.73</td>
          <td class="tg-baqh">77.25</td>
          <td class="tg-baqh">75.57</td>
          <td class="tg-baqh">71.86</td>
        </tr>
      </tbody>
    </table>
    <br>

    We use QuAC and DoQA datasets which have such unanswerable cases to evaluate such capability. We use both answerable and unanswerable samples for this evaluation. Specifically, for unanswerable case, we consider the model indicating that the question cannot be answered as correct, and as for answerable cases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer). In the end, we calculate the average accuracy score of unanswerable and answerable cases as the final metric.

  <h3>Other Evaluations</h3>
    <h4 class="colored_subtitle">KILT</h4>
    <table class="tg">
      <thead>
        <tr>
          <th class="tg-baqh"></th>
          <th class="tg-baqh">&nbsp;&nbsp;&nbsp;Command-R-Plus&nbsp;&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;&nbsp;Llama3-instruct-70b&nbsp;&nbsp;&nbsp;</th>
          <th class="tg-baqh">&nbsp;&nbsp;&nbsp;ChatQA-1.5-70B&nbsp;&nbsp;&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="tg-baqh">&nbsp;&nbsp;Average&nbsp;&nbsp;</td>
          <td class="tg-baqh">71.50</td>
          <td class="tg-baqh">67.57</td>
          <td class="tg-baqh">70.07</td>
        </tr>
      </tbody>
    </table>
    <br>
    The socres are average accuracy over Natural Questions, TriviaQA, and HotpotQA in <a href="https://arxiv.org/abs/2009.02252" class="custom-link">KILT</a> benchmark. We use the same accuracy evaluation metric as in <a href="https://cohere.com/blog/command-r" class="custom-link">Command-R-Plus</a>, where the accuracy is calculated using the presence of keyphrases in the model's answer. We use <a href="https://arxiv.org/abs/2302.07452" class="custom-link">Dragon retriever</a> to retrieve relevant contexts.
    
    <h4 class="colored_subtitle">Human Evaluations</h4>
    <figure>
      <img class="img-humaneval" src="figs/human_eval.png">
    </figure>
    
    In this human evaluation, we randomly select 60 samples from each of 10 datasets in ChatRAG Bench, and each sample is labelled by three annotators, which results in a total of 1800 annotations. We ask annotators to verify the facts in models' outputs and determine which model provides a more accurate response to the question. From the results, ChatQA-1.0-70B and GPT-4 are tie most of the time, and GPT-4 achieves slightly higher win rate.

  <h3>Training Datasets</h3>
    We release the <a href="https://huggingface.co/datasets/nvidia/ChatQA-Training-Data" class="custom-link">training datasets</a> described in <a href="https://arxiv.org/abs/2401.10225" class="custom-link">ChatQA (1.0)</a>.

  <h3>Conversational QA Retriever</h3>
    We release the <a href="https://huggingface.co/nvidia/dragon-multiturn-query-encoder" class="custom-link">dragon multi-turn query retriever</a> for the conversational QA task. The details of this retriever are described in <a href="https://arxiv.org/abs/2401.10225" class="custom-link">ChatQA (1.0)</a>.

  <!-- <h3>License</h3>
    The use of the ChatQA-1.5-8B and ChatQA-1.5-70B are governed by the <a href="https://llama.meta.com/llama3/license/" class="custom-link">META LLAMA 3 COMMUNITY LICENSE AGREEMENT</a>. -->
    
  <h3>Citation</h3>
  <pre><code>@article{liu2024chatqa,
  title={ChatQA: Building GPT-4 Level Conversational QA Models},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2401.10225},
  year={2024}}</code></pre>

  </div>
  
  <br><br><br>

</html>


